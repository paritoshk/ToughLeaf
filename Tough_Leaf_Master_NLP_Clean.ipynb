{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tough_Leaf_Master_NLP_Clean",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paritoshk/ToughLeaf/blob/main/Tough_Leaf_Master_NLP_Clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Finetune GPT-2 on Reddit Data\n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "A variant of the [default notebook](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) optimized for short-form titles. It is recommended to be familiar with that notebook before using this one.\n",
        "\n",
        "This example uses a CSV export of Reddit data via BigQuery (see this post for more information).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a41f557-3d41-4797-98ab-5389db194799"
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2JDlHtOATMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec2e95c7-ae4d-4f8e-c6fb-433f0936b482"
      },
      "source": [
        "pip install tensorflow==1.13.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "  Downloading tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7 MB 9.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.40.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.4.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.37.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.19.5)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.17.3)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.8.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.2) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.7.4.3)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.13.2 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e90f4f-b29e-463c-fc86-f074d7d43afa"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 23 19:14:01 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "The default query returns 1.3MB of data, so probably should only use `124M` GPT-2 to finetune. If working with more Reddity data, then migrate to `355M`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d7f82c9-b7e2-4e02-86be-7739bd562246"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 208Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 1.64Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 247Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:46, 10.6Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 471Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 1.92Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 1.93Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c13265-70fc-470a-9b81-36d945e407f8"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjYhR665GDxA"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKBm0_-dGD1i"
      },
      "source": [
        "df1 = pd.read_excel(r'/content/drive/MyDrive/ToughLeafDatasets/MBE,  WBE - Directory_2021-02-21.xlsx')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "--3veULgGEG1",
        "outputId": "0fb3e2a2-1145-4201-9c72-d5f9a268e1f7"
      },
      "source": [
        "df1.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Company Name</th>\n",
              "      <th>DBA Name</th>\n",
              "      <th>Owner First</th>\n",
              "      <th>Owner Last</th>\n",
              "      <th>Physical Address</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Zip</th>\n",
              "      <th>Mailing Address</th>\n",
              "      <th>City.1</th>\n",
              "      <th>State.1</th>\n",
              "      <th>Zip.1</th>\n",
              "      <th>Phone</th>\n",
              "      <th>Fax</th>\n",
              "      <th>Email</th>\n",
              "      <th>Website</th>\n",
              "      <th>Agency</th>\n",
              "      <th>Certification Type</th>\n",
              "      <th>Certified</th>\n",
              "      <th>Capability</th>\n",
              "      <th>Work Districts/Regions</th>\n",
              "      <th>Industry</th>\n",
              "      <th>Business Size</th>\n",
              "      <th>General Location</th>\n",
              "      <th>Location</th>\n",
              "      <th>Commodity Codes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1 Accord Services Inc.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Helen</td>\n",
              "      <td>Washington</td>\n",
              "      <td>2331 Main Street</td>\n",
              "      <td>Buffalo</td>\n",
              "      <td>NY</td>\n",
              "      <td>14214</td>\n",
              "      <td>PO BOX 922</td>\n",
              "      <td>Buffalo</td>\n",
              "      <td>NY</td>\n",
              "      <td>14205</td>\n",
              "      <td>716-602-2404</td>\n",
              "      <td>716-675-7280</td>\n",
              "      <td>helen@1accordservices.com</td>\n",
              "      <td>http://www.1accordservices.com</td>\n",
              "      <td>NYS</td>\n",
              "      <td>MBE</td>\n",
              "      <td>2016-10-25</td>\n",
              "      <td>BUILDING MAINTENANCE; CARPET CLEANING; CLEANIN...</td>\n",
              "      <td>All work districts/regions</td>\n",
              "      <td>Services Consultants</td>\n",
              "      <td>$100,000 - $499,000</td>\n",
              "      <td>Upstate New York</td>\n",
              "      <td>Western NY</td>\n",
              "      <td>5617 - Services to Buildings and Dwellings; 56...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1 Call Building Maintenance Corp</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lorris</td>\n",
              "      <td>Alleyne</td>\n",
              "      <td>946 Atlantic Ave.</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>NY</td>\n",
              "      <td>11238</td>\n",
              "      <td>946 Atlantic Ave.</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>NY</td>\n",
              "      <td>11238</td>\n",
              "      <td>347-469-0806</td>\n",
              "      <td>516-541-7720</td>\n",
              "      <td>lorrisalleyne1@gmail.com</td>\n",
              "      <td>http://http:www.1callbuildingmaintenance.com</td>\n",
              "      <td>NYS</td>\n",
              "      <td>MBE</td>\n",
              "      <td>2015-10-26</td>\n",
              "      <td>Our Services include Office Cleaning Carpet cl...</td>\n",
              "      <td>Long Island; Mid-Hudson; NYC</td>\n",
              "      <td>Services Consultants</td>\n",
              "      <td>$100,000 - $499,000</td>\n",
              "      <td>Downstate New York</td>\n",
              "      <td>NYC</td>\n",
              "      <td>561720 - Janitorial services; 91009 - Carpet C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1-800 Mr. Rubbish inc</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Abeir</td>\n",
              "      <td>Saleh</td>\n",
              "      <td>1033 86th street</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>NY</td>\n",
              "      <td>11228</td>\n",
              "      <td>1033 86th street</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>NY</td>\n",
              "      <td>11228</td>\n",
              "      <td>800-677-8224</td>\n",
              "      <td>973-732-9145</td>\n",
              "      <td>contactus@1800mrrubbish.com</td>\n",
              "      <td>http://www.1800mrrubbish.com</td>\n",
              "      <td>NYS</td>\n",
              "      <td>WBE</td>\n",
              "      <td>2019-01-31</td>\n",
              "      <td>1-800 Mr. Rubbish prides itself on its zero wa...</td>\n",
              "      <td>All work districts/regions</td>\n",
              "      <td>Construction Consultants</td>\n",
              "      <td>$500,000 - $999,999</td>\n",
              "      <td>Downstate New York</td>\n",
              "      <td>NYC</td>\n",
              "      <td>562111 - Recyclable material collection servic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10 November LLC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Alfred</td>\n",
              "      <td>Reyes</td>\n",
              "      <td>303 Wolfs Lane, Suite 1</td>\n",
              "      <td>Pelham</td>\n",
              "      <td>NY</td>\n",
              "      <td>10803</td>\n",
              "      <td>303 Wolfs Lane, Suite 1</td>\n",
              "      <td>Pelham</td>\n",
              "      <td>NY</td>\n",
              "      <td>10803</td>\n",
              "      <td>914-885-1515</td>\n",
              "      <td>914-885-0074</td>\n",
              "      <td>a.reyes@10novgroup.com</td>\n",
              "      <td>http://www.10novgroup.com</td>\n",
              "      <td>NYS</td>\n",
              "      <td>MBE</td>\n",
              "      <td>2018-08-07</td>\n",
              "      <td>Carpentry, masonry, concrete, finishing, repai...</td>\n",
              "      <td>Long Island; NYC</td>\n",
              "      <td>Construction</td>\n",
              "      <td>$100,000 - $499,000</td>\n",
              "      <td>Downstate New York</td>\n",
              "      <td>NYC</td>\n",
              "      <td>425120 - Wholesale Trade Agents and Brokers; 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1020 Let's Go!</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Karen</td>\n",
              "      <td>Eason</td>\n",
              "      <td>550 Manor Road, #141148</td>\n",
              "      <td>Staten Island</td>\n",
              "      <td>NY</td>\n",
              "      <td>10314</td>\n",
              "      <td>550 Manor Road, #141148</td>\n",
              "      <td>Staten Island</td>\n",
              "      <td>NY</td>\n",
              "      <td>10314</td>\n",
              "      <td>718-873-7599</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ke@1020letsgo.com</td>\n",
              "      <td>1020letsgo.com</td>\n",
              "      <td>NYS</td>\n",
              "      <td>MBE</td>\n",
              "      <td>2019-01-22</td>\n",
              "      <td>Assist/teach startup and emerging entrepreneur...</td>\n",
              "      <td>All work districts/regions</td>\n",
              "      <td>Services Consultants</td>\n",
              "      <td>Less than $100,000</td>\n",
              "      <td>Downstate New York</td>\n",
              "      <td>NYC</td>\n",
              "      <td>541611 - Administrative Management and General...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Company Name  ...                                    Commodity Codes\n",
              "0            1 Accord Services Inc.  ...  5617 - Services to Buildings and Dwellings; 56...\n",
              "1  1 Call Building Maintenance Corp  ...  561720 - Janitorial services; 91009 - Carpet C...\n",
              "2             1-800 Mr. Rubbish inc  ...  562111 - Recyclable material collection servic...\n",
              "3                   10 November LLC  ...  425120 - Wholesale Trade Agents and Brokers; 9...\n",
              "4                    1020 Let's Go!  ...  541611 - Administrative Management and General...\n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uirREvn4GEK1"
      },
      "source": [
        "commodity_list_train = list(df1['Commodity Codes'])\n",
        "capablity_list_train = list(df1['Capability'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr5eYpQWG2gy"
      },
      "source": [
        "import re\n",
        "commoditytest_split_words = [] \n",
        "for i in commodity_list_train:\n",
        "    i = str(i)\n",
        "    i = re.sub(r\"(^|\\W)\\d+\", \" \", i)\n",
        "    i = re.sub(r\"-\", \" \", i)\n",
        "    commoditytest_split_words.append(i)\n",
        "capablitytest_split_words = [] \n",
        "for i in capablity_list_train:\n",
        "    i = str(i)\n",
        "    i = re.sub(r\"(^|\\W)\\d+\", \" \", i)\n",
        "    i = re.sub(r\"-\", \" \", i)\n",
        "    capablitytest_split_words.append(i)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zc3ee6QG2kK",
        "outputId": "1929bac7-a392-401c-894c-00be6bf6752c"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "A single-column CSV is expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "file_name = \"results-20190831-145056.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS"
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "Providing a single-column CSV will automatically add `<|startoftext|>` and `<|endoftext|>` tokens appropriately.\n",
        "\n",
        "Short form text is more likely to overfit, so train it with fewer steps than you would for longform content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9c9c816-b7a3-4068-9dcc-1ef0cc55b62b"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='774M',\n",
        "              steps=500,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=100\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0902 01:24:59.877743 139841164683136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0902 01:25:14.827409 139841164683136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 30.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 518272 tokens\n",
            "Training...\n",
            "[10 | 28.56] loss=1.72 avg=1.72\n",
            "[20 | 52.24] loss=1.65 avg=1.69\n",
            "[30 | 77.31] loss=1.61 avg=1.66\n",
            "[40 | 101.84] loss=1.60 avg=1.64\n",
            "[50 | 125.96] loss=1.58 avg=1.63\n",
            "[60 | 150.46] loss=1.56 avg=1.62\n",
            "[70 | 175.09] loss=1.40 avg=1.59\n",
            "[80 | 199.58] loss=1.46 avg=1.57\n",
            "[90 | 224.08] loss=1.33 avg=1.54\n",
            "[100 | 248.60] loss=1.48 avg=1.53\n",
            "======== SAMPLE 1 ========\n",
            " use the word. Do you want to be a murderer?<|endoftext|>\n",
            "<|startoftext|>what happens in the universe where humans have no food?<|endoftext|>\n",
            "<|startoftext|>What’s your worst, but the coolest stuff to wear?<|endoftext|>\n",
            "<|startoftext|>Without naming it in the title, how is your life today different than it was in the 90’s?<|endoftext|>\n",
            "<|startoftext|>People who have been to bed before 9/11. What were your very obvious signs of a real, real threat?<|endoftext|>\n",
            "<|startoftext|>Whats happened to your roommate that surprised you?<|endoftext|>\n",
            "<|startoftext|>Who’s the greatest teacher/student ever found out?<|endoftext|>\n",
            "<|startoftext|>What's the dumbest thing you did during elementary school?<|endoftext|>\n",
            "<|startoftext|>What's something you've not forgotten?<|endoftext|>\n",
            "<|startoftext|>What would you recommend to everyone that you have had with a friend, family or employer?<|endoftext|>\n",
            "<|startoftext|>What is the worst time to break out?<|endoftext|>\n",
            "<|startoftext|>What's the funniest thing you've ever said in a conversation?<|endoftext|>\n",
            "<|startoftext|>What is the closest your pet is to a dog right now?<|endoftext|>\n",
            "<|startoftext|>What do you think about a woman?<|endoftext|>\n",
            "<|startoftext|>What would happen if a child raised with an incestuous parent(s) was raised with an adopted parent; how would it react?<|endoftext|>\n",
            "<|startoftext|>Hey Reddit. What happened on your reddit post, when you were posting?<|endoftext|>\n",
            "<|startoftext|>What is the scariest thing your parents have done, what was it as a kid?<|endoftext|>\n",
            "<|startoftext|>What's a phrase that you can swear people don’t understand?<|endoftext|>\n",
            "<|startoftext|>What’s so bad that people like it?<|endoftext|>\n",
            "<|startoftext|>Without naming it in the title, what is your favourite word?<|endoftext|>\n",
            "<|startoftext|>What's the stupidest argument a politician uses?<|endoftext|>\n",
            "<|startoftext|>People of Reddit who grew up with the old dad of your life (if not your grandfather), what is the biggest mistake you've made?<|endoftext|>\n",
            "<|startoftext|>What is a little too good to be true?<|endoftext|>\n",
            "<|startoftext|>What is something you still haven't seen/experienced that would be very surprising with no knowledge or experience?<|endoftext|>\n",
            "<|startoftext|>If you could get every person on board for one year to see who's the best person in the world, who would you choose?<|endoftext|>\n",
            "<|startoftext|>[serious] Reddit, what is the one small thing that changed your life?<|endoftext|>\n",
            "<|startoftext|>If you had the privilege of living in the future (the present world), what would the dystopian future look like?<|endoftext|>\n",
            "<|startoftext|>[Serious] What's the worst case of porn ever?<|endoftext|>\n",
            "<|startoftext|>What would you put on a cake day to show off your crush?<|endoftext|>\n",
            "<|startoftext|>If you could get the best deal on a new car each year, what would you choose?<|endoftext|>\n",
            "<|startoftext|>Gamers of reddit, what is the most useless item that anyone can pick at random that gets stuck on you?<|endoftext|>\n",
            "<|startoftext|>Redditors who use your sex symbol, what is your opinion?<|endoftext|>\n",
            "<|startoftext|>It sounds like you've got the same problem as the whole population. What does your problem look like?<|endoftext|>\n",
            "<|startoftext|>What is a\n",
            "\n",
            "[110 | 284.94] loss=1.50 avg=1.53\n",
            "[120 | 309.46] loss=1.39 avg=1.52\n",
            "[130 | 333.99] loss=1.58 avg=1.52\n",
            "[140 | 358.49] loss=1.33 avg=1.51\n",
            "[150 | 383.00] loss=1.44 avg=1.50\n",
            "[160 | 407.50] loss=1.51 avg=1.50\n",
            "[170 | 432.02] loss=1.22 avg=1.49\n",
            "[180 | 456.53] loss=1.24 avg=1.47\n",
            "[190 | 481.04] loss=1.32 avg=1.46\n",
            "[200 | 505.56] loss=1.20 avg=1.45\n",
            "======== SAMPLE 1 ========\n",
            "'s death, you want to find out why the devil is here. What's your theory?<|endoftext|>\n",
            "<|startoftext|>Reddit, Whats the weirdest story you have heard people have about you, and how you got the whole thing to begin with?<|endoftext|>\n",
            "<|startoftext|>For the non-English speaking Redditors, how did you meet your goal of becoming a doctor?<|endoftext|>\n",
            "<|startoftext|>What’s a movie you can’t wait to see?<|endoftext|>\n",
            "<|startoftext|>What hobby has kept you going to the point where you no longer need to worry about anything?<|endoftext|>\n",
            "<|startoftext|>What do you do on Valentine's Day and what date do you bring back from the grave?<|endoftext|>\n",
            "<|startoftext|>What were you surprised you missed out on?<|endoftext|>\n",
            "<|startoftext|>You die/end life support. What do you do?<|endoftext|>\n",
            "<|startoftext|>What does every country do that pisses you off more than it deserves?<|endoftext|>\n",
            "<|startoftext|>If you could have any subreddit, what would it be ?<|endoftext|>\n",
            "<|startoftext|>The fact is, all food is toxic to the human gut. What food do you want to eat?<|endoftext|>\n",
            "<|startoftext|>What's the strangest way you've got sexed up?<|endoftext|>\n",
            "<|startoftext|>What is the most amazing thing you've found ?<|endoftext|>\n",
            "<|startoftext|>The world is now a zombie infested planet overrun by new/uninfected humans. What do you get out of it?<|endoftext|>\n",
            "<|startoftext|>You are given a choice between a lifetime supply of a single drug, and 1 of two other drugs. For the rest of your life you get unlimited supply. Which drug will you choose?<|endoftext|>\n",
            "<|startoftext|>What's something that doesn’t get paid enough?<|endoftext|>\n",
            "<|startoftext|>What was your \"I'm a useless fuck up\" moment?<|endoftext|>\n",
            "<|startoftext|>If people found out you were cheating on your spouse, what would you do?<|endoftext|>\n",
            "<|startoftext|>Where is the strangest place you've ever been?<|endoftext|>\n",
            "<|startoftext|>Why? For 1/12th of a inch of the screen. You can tell that your phone is being listened to because a nearby TV is taking a huge view of you. What's your story?<|endoftext|>\n",
            "<|startoftext|>Who's the biggest douchebag in the movie?<|endoftext|>\n",
            "<|startoftext|>For the second time in a month, Bob Ross turns down an invite from TV to guest-star in his movie \"Private Investigators\". What do you choose?<|endoftext|>\n",
            "<|startoftext|>What's something you don't understand about your SO that makes them special?<|endoftext|>\n",
            "<|startoftext|>What are your favourite podcasts that can be listened to 24/7?<|endoftext|>\n",
            "<|startoftext|>Whats the dumbest/creepiest thing you’ve heard in a school trip?<|endoftext|>\n",
            "<|startoftext|>what is the worst thing that has happened to you in the last 10 years?<|endoftext|>\n",
            "<|startoftext|>What’s the saddest thought you’ve ever had?<|endoftext|>\n",
            "<|startoftext|>What are some cool free apps to have in your phone's settings?<|endoftext|>\n",
            "<|startoftext|>What is something that annoys you, just because you look good doing it?<|endoftext|>\n",
            "<|startoftext|>What's something you think you've found out is actually real?<|endoftext|>\n",
            "<|startoftext|>If a person were to turn into a dinosaur and had the ability to punch you in the face with his hands what would his face look like?<|endoftext|>\n",
            "<|startof\n",
            "\n",
            "[210 | 540.66] loss=1.34 avg=1.44\n",
            "[220 | 565.18] loss=1.39 avg=1.44\n",
            "[230 | 589.70] loss=1.26 avg=1.43\n",
            "[240 | 614.21] loss=1.25 avg=1.42\n",
            "[250 | 638.72] loss=1.29 avg=1.42\n",
            "[260 | 663.23] loss=1.23 avg=1.41\n",
            "[270 | 687.80] loss=1.23 avg=1.40\n",
            "[280 | 712.31] loss=1.23 avg=1.39\n",
            "[290 | 736.82] loss=1.15 avg=1.38\n",
            "[300 | 761.32] loss=1.08 avg=1.37\n",
            "======== SAMPLE 1 ========\n",
            "<|startoftext|>How many people did you not like when you were younger?<|endoftext|>\n",
            "<|startoftext|>Redditors who work abroad: What are some of your most interesting stories from your workplace, or to find out more about your country/ region/ region of origin?<|endoftext|>\n",
            "<|startoftext|>What is something that you find creepy about the opposite sex of the same sex?<|endoftext|>\n",
            "<|startoftext|>When it's your turn to tell the ending, do you choose your words or act?<|endoftext|>\n",
            "<|startoftext|>What job would be extremely boring without being an asshole?<|endoftext|>\n",
            "<|startoftext|>What's the most inappropriate thing a teacher has ever asked you to do or say?<|endoftext|>\n",
            "<|startoftext|>What are the best subs that are both good and bad?<|endoftext|>\n",
            "<|startoftext|>What's the most fucked up place you've had sex?<|endoftext|>\n",
            "<|startoftext|>What's your “I'll never forget” story?<|endoftext|>\n",
            "<|startoftext|>What's the longest that I've had to sleep through?<|endoftext|>\n",
            "<|startoftext|>Hey Reddit, what is your favorite video game but never played?<|endoftext|>\n",
            "<|startoftext|>What is your #1 book to read to someone when they are tired and sleepy?<|endoftext|>\n",
            "<|startoftext|>What's your \"I swear this is really old\" story or story that someone can tell to make them think other people are older?<|endoftext|>\n",
            "<|startoftext|>What’s the most useful tool someone ever asked for that never came into existence?<|endoftext|>\n",
            "<|startoftext|>What's the most fucked up thing you've ever seen at someone’s home?<|endoftext|>\n",
            "<|startoftext|>What's the biggest mistake you made as a kid?<|endoftext|>\n",
            "<|startoftext|>In your opinion, what is the best cereal to make pancakes?<|endoftext|>\n",
            "<|startoftext|>What's something you were surprised to never grow up thinking about until you've grown up?<|endoftext|>\n",
            "<|startoftext|>What was your earliest memory as a kid?<|endoftext|>\n",
            "<|startoftext|>Where is the creepiest place someone would have sex?<|endoftext|>\n",
            "<|startoftext|>What's a uniquely British thing that Canadians really miss?<|endoftext|>\n",
            "<|startoftext|>What's your worst Reddit post yet (not a joke, just a horrible one)?<|endoftext|>\n",
            "<|startoftext|>[SERIOUS]what’s your go to playlist once you've finished drinking?<|endoftext|>\n",
            "<|startoftext|>People who have done one type of porn for 3 months: how did you stop?<|endoftext|>\n",
            "<|startoftext|>What was your favorite TV show that ended/didn't go away?<|endoftext|>\n",
            "<|startoftext|>If you are offered $5 million to watch porn for 3 months straight, which one do you pick and why?<|endoftext|>\n",
            "<|startoftext|>What is the most underrated TV show of all time?<|endoftext|>\n",
            "<|startoftext|>What’s your best advice to someone going through a \"catastrophic breakdown\"?<|endoftext|>\n",
            "<|startoftext|>What is the worst thing someone could say to you when you weren't around anymore?<|endoftext|>\n",
            "<|startoftext|>People who've never had sex, how long have you had sex, did it happen regularly before or after marriage, and if so how did it affect your relationship?<|endoftext|>\n",
            "<|startoftext|>What’s a word/phrase everyone should think they know and sound like they understand ?<|endoftext|>\n",
            "<|startoftext|>What job should be reserved for \"old folks\". Jobs in the military, for example, are dominated by older people. If someone who has worked in the military can't believe it's existance, then why does\n",
            "\n",
            "[310 | 796.45] loss=1.18 avg=1.37\n",
            "[320 | 821.00] loss=1.13 avg=1.36\n",
            "[330 | 845.52] loss=1.05 avg=1.35\n",
            "[340 | 870.05] loss=1.14 avg=1.34\n",
            "[350 | 894.66] loss=1.05 avg=1.33\n",
            "[360 | 919.17] loss=1.02 avg=1.32\n",
            "[370 | 943.70] loss=1.20 avg=1.31\n",
            "[380 | 968.21] loss=0.95 avg=1.30\n",
            "[390 | 992.74] loss=0.90 avg=1.29\n",
            "[400 | 1017.26] loss=0.94 avg=1.28\n",
            "======== SAMPLE 1 ========\n",
            "? “What are some “tips, tricks, or instructions” things that people don’t realize are actually nonces?<|endoftext|>\n",
            "<|startoftext|>What's a skill a person has that no longer exists?<|endoftext|>\n",
            "<|startoftext|>What is your favorite movie?<|endoftext|>\n",
            "<|startoftext|>What is your most overused thought?<|endoftext|>\n",
            "<|startoftext|>What would be impossible to have a bad day?<|endoftext|>\n",
            "<|startoftext|>[Serious] What are some good non-humorous Wikipedia articles to have on your CV?<|endoftext|>\n",
            "<|startoftext|>People who voted for Trump, what does he instill in you?<|endoftext|>\n",
            "<|startoftext|>What is an unusual sexual response you've ever heard?<|endoftext|>\n",
            "<|startoftext|>What is a subreddit you should know about?<|endoftext|>\n",
            "<|startoftext|>If life were a video game, what would some missions be in the game ?<|endoftext|>\n",
            "<|startoftext|>If a genie grants you the chance to go to any event/situation to be your genie, but you cannot do anything for the genie, what would you do?<|endoftext|>\n",
            "<|startoftext|>You're on a date with your closest friend, who is the man and why is it your friend?<|endoftext|>\n",
            "<|startoftext|>What do you think about the belief that your food’s name should be synonymous with its actual ingredient?<|endoftext|>\n",
            "<|startoftext|>What do you have for dinner tonight?<|endoftext|>\n",
            "<|startoftext|>What sounds like a scam, yet is actually a scam?<|endoftext|>\n",
            "<|startoftext|>How are you doing?<|endoftext|>\n",
            "<|startoftext|>Men of Reddit, have you noticed that your chest pain becomes more pronounced the more you’re in pain? Do you guys use a meds before or after a day to ease it off?<|endoftext|>\n",
            "<|startoftext|>What screams \"I was born on February 4th?\" If you knew you’re going to be born on February 4, what would you do?<|endoftext|>\n",
            "<|startoftext|>Why is it so difficult for Asian Americans to get jobs where we will resent us and work hard to live down the middle of our income bracket, even though we won't mind paying close to nothing for them and we'll do most of the work for them?<|endoftext|>\n",
            "<|startoftext|>Redditors, in your experience with the job, what is your opinion on the current generation that are painting and video-mapping President Trump?<|endoftext|>\n",
            "<|startoftext|>What is the one thing you hate most about Reddit?<|endoftext|>\n",
            "<|startoftext|>What’s a quote that you still use regularly?<|endoftext|>\n",
            "<|startoftext|>What is the worst pain you’ve ever felt in your life?<|endoftext|>\n",
            "<|startoftext|>What's your favorite cartoon?<|endoftext|>\n",
            "<|startoftext|>What's something we’ll eventually lose for good?<|endoftext|>\n",
            "<|startoftext|>Parents of reddit, when was the last time you realized your child is a monster?<|endoftext|>\n",
            "<|startoftext|>What would happen if you had to be an actor in a videogame?<|endoftext|>\n",
            "<|startoftext|>What is your favorite memory of all time?<|endoftext|>\n",
            "<|startoftext|>What is something you are still unsure about but totally agree with?<|endoftext|>\n",
            "<|startoftext|>What are you excited to announce?<|endoftext|>\n",
            "<|startoftext|>Americans of Reddit, what’s your country’s worst government image?<|endoftext|>\n",
            "<|startoftext|>Teachers of Reddit, what's your pick/ choose between a teacher and an applicant?<|endoftext|>\n",
            "<|startoftext|>\n",
            "\n",
            "[410 | 1052.38] loss=1.01 avg=1.27\n",
            "[420 | 1076.89] loss=1.01 avg=1.26\n",
            "[430 | 1101.44] loss=0.92 avg=1.25\n",
            "[440 | 1126.05] loss=0.85 avg=1.24\n",
            "[450 | 1150.56] loss=0.86 avg=1.23\n",
            "[460 | 1175.07] loss=0.96 avg=1.23\n",
            "[470 | 1199.59] loss=0.83 avg=1.22\n",
            "[480 | 1224.14] loss=1.06 avg=1.21\n",
            "[490 | 1248.81] loss=0.95 avg=1.20\n",
            "[500 | 1273.42] loss=0.86 avg=1.20\n",
            "Saving checkpoint/run1/model-500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Load a Trained Model Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "Same as normal generate functions, except with additional parameters to handle the new tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "58344d45-7ac8-4f0e-be97-634cf9fbd8c8"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1',\n",
        "             length=100,\n",
        "             prefix=\"<|startoftext|>\",\n",
        "             truncate=\"<|endoftext|>\",\n",
        "             include_prefix=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Why did you start using Reddit?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "8e6049b8-56ba-4d9c-e127-da454cc81ac6"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=100,\n",
        "              temperature=1.0,\n",
        "              nsamples=10,\n",
        "              batch_size=10,\n",
        "              prefix=\"<|startoftext|>\",\n",
        "              truncate=\"<|endoftext|>\",\n",
        "              include_prefix=False\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What Is YOUR best advice for keeping your peace and quiet?\n",
            "====================\n",
            "According to you, what is the most beautiful country in the world?\n",
            "====================\n",
            "What put you in the E.R. ?\n",
            "====================\n",
            "What is a question you really want answered, but not enough to waste time answering?\n",
            "====================\n",
            "What was the zone of the White House where you would see the President and your pet at?\n",
            "====================\n",
            "What is something you always wish someone would do for you?\n",
            "====================\n",
            "What do you do to balance your busy life and save time?\n",
            "====================\n",
            "As a kid, what did you want to be when you were younger?\n",
            "====================\n",
            "If you could see every time the same color combination in a movie where you would be led to believe so?\n",
            "====================\n",
            "What is the laziest thing you did on your cake day?\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP7iHAhCAEKo"
      },
      "source": [
        "If generating in bulk, you may want to set `sample_demin=''` to remove the delimiter between each sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=100,\n",
        "                      temperature=1.0,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20,\n",
        "                      prefix=\"<|startoftext|>\",\n",
        "                      truncate=\"<|endoftext|>\",\n",
        "                      include_prefix=False,\n",
        "                      sample_delim=''\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g"
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}